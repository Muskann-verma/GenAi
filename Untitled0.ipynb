{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPkcsfdaZbtAoLTuDV1dZ+Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Muskann-verma/GenAi/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDl7X4zCF31m",
        "outputId": "e0d4250f-3150-4507-d289-4e56c012bf92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip  install  nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('brown')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RT_uVYB9Luep",
        "outputId": "31189ce6-d4d8-41aa-98f6-911099d36b77"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXPERIMENT 1:"
      ],
      "metadata": {
        "id": "YhM3P96wP5Rk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('brown')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGEDoPPRP18u",
        "outputId": "ca873f1e-271d-46a7-a8d0-0410e32f5484"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import brown"
      ],
      "metadata": {
        "id": "k7S_x4CgQI0S"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categories = brown.categories()\n",
        "print(categories)\n",
        "print(\"total categories:\",len(categories))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZX0eC38aQj-x",
        "outputId": "8b6c2a61-3aad-4dc5-ee90-7a39f626fa7c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n",
            "total categories: 15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "brown.words(categories='adventure')[:50]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSEGQMqAQwcH",
        "outputId": "55c847a0-d620-4426-fc4f-fc2116c62c6c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Dan',\n",
              " 'Morgan',\n",
              " 'told',\n",
              " 'himself',\n",
              " 'he',\n",
              " 'would',\n",
              " 'forget',\n",
              " 'Ann',\n",
              " 'Turner',\n",
              " '.',\n",
              " 'He',\n",
              " 'was',\n",
              " 'well',\n",
              " 'rid',\n",
              " 'of',\n",
              " 'her',\n",
              " '.',\n",
              " 'He',\n",
              " 'certainly',\n",
              " \"didn't\",\n",
              " 'want',\n",
              " 'a',\n",
              " 'wife',\n",
              " 'who',\n",
              " 'was',\n",
              " 'fickle',\n",
              " 'as',\n",
              " 'Ann',\n",
              " '.',\n",
              " 'If',\n",
              " 'he',\n",
              " 'had',\n",
              " 'married',\n",
              " 'her',\n",
              " ',',\n",
              " \"he'd\",\n",
              " 'have',\n",
              " 'been',\n",
              " 'asking',\n",
              " 'for',\n",
              " 'trouble',\n",
              " '.',\n",
              " 'But',\n",
              " 'all',\n",
              " 'of',\n",
              " 'this',\n",
              " 'was',\n",
              " 'rationalization',\n",
              " '.',\n",
              " 'Sometimes']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  EXPERIMENT 2:\n",
        "Write the code to perform basic text processing task. The program should create a corpus using user defined words, strings txt files and generate a vocabulary from the corpus. It should then clean the corpus by removing all stopwords. After cleaning, the program must saved and generate vocabulary in JSON format."
      ],
      "metadata": {
        "id": "uHOY7ZpuGD4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKLr_KD-HMf9",
        "outputId": "30b7abf1-f575-4316-8a8a-6fe88ab1a764"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement json (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for json\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "17zLLRUVF_zg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import json\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "BC6Oxbj5GOgs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrhlpvgZIDfD",
        "outputId": "b079eeb3-768c-425e-cad8-7a20650ac806"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text =\"\"\"\n",
        "my name is muskan verma and i am from lucknow and currently i am persuing btech in csiot from psit kanpur\"\"\"\n",
        "corpus = text.lower()\n",
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZbOK_o8IMa7",
        "outputId": "af7be827-8e45-4c62-82c1-754f74ca2081"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "my name is is is is  muskan verma and i am from lucknow and currently i am persuing btech in csiot from psit kanpur\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')\n",
        "tokens = word_tokenize(corpus)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CyxUeD0IpIE",
        "outputId": "1557c31c-c60e-43e5-fbda-48ebd738fa5f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['my', 'name', 'is', 'muskan', 'verma', 'and', 'i', 'am', 'from', 'lucknow', 'and', 'currently', 'i', 'am', 'persuing', 'btech', 'in', 'csiot', 'from', 'psit', 'kanpur']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "clean_tokens = [word for word in tokens if word not in stop_words]\n",
        "print(clean_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTB8TeEbIou4",
        "outputId": "50401c54-8f82-4eb9-9ad9-05c1598e6db2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['name', 'muskan', 'verma', 'lucknow', 'currently', 'persuing', 'btech', 'csiot', 'psit', 'kanpur']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = sorted(set(clean_tokens))\n",
        "print(vocabulary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOhdtGfMJfQh",
        "outputId": "6f22a8e2-8bdd-4660-bb5c-af54149b1a6d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['btech', 'csiot', 'currently', 'kanpur', 'lucknow', 'muskan', 'name', 'persuing', 'psit', 'verma']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_dict = {\"vocabulary\": vocabulary}\n",
        "with open(\"vocabulary.json\",\"w\") as file:\n",
        "  json.dump(vocab_dict,file,indent=4)\n",
        "print(\"vocabulary saved to vocabulary.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Y1wxtAPKEfL",
        "outputId": "50aa533d-4721-4f8c-c4b6-72ecb8088441"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocabulary saved to vocabulary.json\n"
          ]
        }
      ]
    }
  ]
}